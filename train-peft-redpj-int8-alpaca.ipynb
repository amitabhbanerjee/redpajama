{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning RedPajama (\"OpenLlama\")\n",
    "by John Robinson 05/15/2023 [Follow @johnrobinsn on Twitter](https://twitter.com/johnrobinsn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://www.storminthecastle.com/img/github.svg\">](https://github.com/johnrobinsn/ObjectDetectionNotebooks/blob/main/nbs/05_ssd_final.ipynb) [<img src=\"https://www.storminthecastle.com/img/colab.svg\">](https://colab.research.google.com/github/johnrobinsn/ObjectDetectionNotebooks/blob/main/nbs/05_ssd_final.ipynb)\n",
    "\n",
    "[Llama](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/), the large language model from Meta was [_effectively_ released](https://github.com/facebookresearch/llama/pull/73/files) to the world a little over two months ago, resulting in an explosion of [experimentation](https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor), [exploration](https://lmsys.org/blog/2023-03-30-vicuna/) and [innovation](https://github.com/ggerganov/llama.cpp). Unfortunately Llama has a fairly restrictive \"research-only\" license which prohibits commercial use.  \n",
    "\n",
    "[RedPajama](https://www.together.xyz/blog/redpajama) is an effort to create an open-source clone of Llama.  It's a project to create leading open-source language models and starts by reproducing the LLaMA training dataset of over 1.2 trillion tokens. Leveraging this dataset, the team from [together.xyz](http://together.xyz) has just released initial versions of their 3B and 7B RedPajama models. Both of these come in a number of flavors including a [base model](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1) and a couple of finetuned variants (an instruction-tuned variant and a chat-tuned variant). **All of these have been released under a liberal Apache-2.0 Open Source License. Making them suitable for commercial applications**\n",
    "\n",
    "Out of the box, the base model (given a sequence of words) has been trained to \"statistically\" predict what word should come next based on the very large dataset described above. As such, the language model has in a way compressed and encoded much of the information contained within that dataset and when prompted can regenerate information that it has learned, one token at a time. These generated sequences echo many of the ideas, concepts and \"thoughts\" that were encoded in the dataset, reflecting thoughts about society, humanity, general knowledge and more. You can sort of think of the base model as having compressed a large portion of the Internet into a set of a few billion numbers.  \n",
    "\n",
    "But while predicting what word comes next is a powerful capability, it is fairly limited in practical application. Enter the process of alignment or finetuning. As you might imagine, the base model has been exposed to many different patterns of written language, ranging from freeform prose, to poetry, Q&As, dialogue etc. Finetuning is about bringing one or more of these latent language patterns to the surface, so that the language model performs better at a specific desired behavior or task. A few of the more common downstream tasks that large language models have been proven to be good at include:\n",
    "\n",
    "* Document Summarization\n",
    "* Sentiment Analysis\n",
    "* Chatbot (Human/Bot. Something like ChatGPT)\n",
    "* Instruction Following\n",
    "\n",
    "In this notebook, I'll demonstrate finetuning the RedPJ model into an \"Instruction Following\" variant using the [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) dataset. Once RedPJ has been finetuned for the task of Instruction Following, we'll expect our model to be able to take in a natural language instruction and be able to generate coherent responses to those instructions.  My goal here is two-fold, one to demonstrate the mechanics of how to finetune a LLM on a dataset of your own choosing and two to show the profound impact that finetuning can have on the behavior of a LLM.\n",
    "\n",
    "There are several approaches that we can take to finetuning an LLM model. Full finetuning involves using back propogation to iteratively modify all of the model weights which can be quite resource intensive from a compute and a memory perspective. Another popular approach to finetuning is through the use of [LoRA adapters](https://arxiv.org/abs/2106.09685).  LoRA is a technique that allows you to finetune a large language model on a much smaller GPU. It does this by freezing the base model weights and dynamically injecting trainable \"adapter\" layers into the model. The number of trainable parameters in these adapter layers are much much smaller than the base model. We'll also use [int8 quantization](https://arxiv.org/abs/2208.07339) of the base model weights to further reduce the amount of memory needed for training.\n",
    "\n",
    "One additional benefit, since LoRA adapters are a separate set of weights from the base model and are much smaller. You can have just one copy of the large base model weights and dynamically load different LoRA adapters weights to support multiple downstream tasks for the same shared base model weights.\n",
    "\n",
    "Later in this article, I'll demonstrate interacting with the base model before and after finetuning, so that you can see the effects of finetuning more clearly.\n",
    "\n",
    "The HuggingFace ecosystem, makes downloading, training, saving models and datasets very quick and easy. This notebook heavily leverages the HuggingFace libraries and platform.  \n",
    "\n",
    "_Note: This notebook should support any of the upcoming RedPajama model releases (bigger ones are coming... assuming you have enough VRAM on your GPU). I've trained and tested this notebook on both the 3B and 7B models that have been released so far._\n",
    "\n",
    "Required VRAM\n",
    "* The 3B model should require less than 8G of VRAM for training.\n",
    "* The 7B model should require less than 12G of VRAM for training.\n",
    "\n",
    "We can start by defining a few variables that will determine which specific base model that we're finetuning (either the 3B or the 7B model)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Configure the base model and a few other variables that we'll use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('togethercomputer/RedPajama-INCITE-Base-7B-v0.1',\n",
       " 'johnrobinsn/alpaca-cleaned',\n",
       " 'redpj7B-lora-int8-alpaca',\n",
       " 'redpj7B-lora-int8-alpaca')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = '3B' #'7B' # Pick your poison\n",
    "\n",
    "if model == '7B':\n",
    "    model_name = (\"togethercomputer/RedPajama-INCITE-Base-7B-v0.1\",\"togethercomputer/RedPajama-INCITE-Base-7B-v0.1\")\n",
    "    run_name = 'redpj7B-lora-int8-alpaca'\n",
    "    dataset = 'johnrobinsn/alpaca-cleaned'\n",
    "    peft_name = 'redpj7B-lora-int8-alpaca'\n",
    "    output_dir = 'redpj7B-lora-int8-alpaca-results'\n",
    "else: #3B\n",
    "    model_name = (\"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\"togethercomputer/RedPajama-INCITE-Base-3B-v1\")\n",
    "    run_name = 'redpj3B-lora-int8-alpaca'\n",
    "    dataset = 'johnrobinsn/alpaca-cleaned'\n",
    "    peft_name = 'redpj3B-lora-int8-alpaca'\n",
    "    output_dir = 'redpj3B-lora-int8-alpaca-results'\n",
    "\n",
    "model_name[1],dataset,peft_name,run_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "def install_dependencies():\n",
    "    !pip install -Uqq  git+https://github.com/huggingface/peft.git\n",
    "    !pip install -Uqq \"transformers==4.27.1\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" loralib --upgrade --quiet\n",
    "    !pip install -Uqq wandb\n",
    "    !pip install -Uqq protobuf==3.20\n",
    "\n",
    "# uncomment the following line to install the required dependencies\n",
    "install_dependencies()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note: If you just want to do inference you can jump all the way down to the [\"Evaluate\"](#evaluate) cell and start running from there to download my adapter weights from HF hub and try some prompts through the finetuned model.__\n",
    "\n",
    "But if you want to train keep going... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Tracking and Monitoring using Weights and Biases\n",
    "\n",
    "This notebook has support for logging the training run to [weights and biases (wandb)](https://wandb.ai/site).  This makes it very easy to track, monitor and annotate your training sessions from anywhere.  \n",
    "\n",
    "Run the next cell and follow the directions given to authenticate with wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjohnrobinsn\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "report_to = \"wandb\" # \"none\"\n",
    "\n",
    "if report_to != \"none\":\n",
    "    import wandb\n",
    "    wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After authenticating, we have to initialize wandb.  We add a few key-value pairs about the run to the information that will be logged to the wandb dashboard.  \n",
    "\n",
    "_Note: You can add more key/values if you'd like._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt2/code/redpj_nbs/3b/wandb/run-20230514_202008-y0fp0eat</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/johnrobinsn/redpj7B-lora-int8-alpaca/runs/y0fp0eat' target=\"_blank\">hopeful-butterfly-19</a></strong> to <a href='https://wandb.ai/johnrobinsn/redpj7B-lora-int8-alpaca' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/johnrobinsn/redpj7B-lora-int8-alpaca' target=\"_blank\">https://wandb.ai/johnrobinsn/redpj7B-lora-int8-alpaca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/johnrobinsn/redpj7B-lora-int8-alpaca/runs/y0fp0eat' target=\"_blank\">https://wandb.ai/johnrobinsn/redpj7B-lora-int8-alpaca/runs/y0fp0eat</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/johnrobinsn/redpj7B-lora-int8-alpaca/runs/y0fp0eat?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f2c238fb130>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=run_name,config={\n",
    "    \"model\": model_name[1],\n",
    "    \"dataset\":dataset\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you get training started below.  You can revisit the wandb links shown above to monitor the status of your training run from anywhere with Internet connectivity.  \n",
    "\n",
    "_Note: I like to send the link (View run) to my phone so that I can monitor on the go..._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "The tokenizer converts words into a list/tensor of numbers so that the model can process them.  Each language model has been trained using a specific tokenizer.  If your base model is already supported by huggingface then the transformer library makes it very easy to load the correct tokenizer for your given model.  Just use the AutoTokenizer class to create an instance of the correct tokenizer by just specifying the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for model:  togethercomputer/RedPajama-INCITE-Base-7B-v0.1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"Loading tokenizer for model: \", model_name[1])\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name[1],add_eos_token=True)\n",
    "tokenizer.pad_token_id = 0  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem that I've found with many of the finetuning scripts and notebooks found online is that the \"end-of-stream\" handling is not done correctly, so in many cases the finetuned models don't know when to stop emitting tokens and tend to \"blabber\" on.  Since we are finetuning on an instruction following task, we would like the model to respond to the instruction prompt succintly and then stop.  There are a number of ways to approach this, but the way I approach it here is to explicitly add a new token to represent end-of-stream, &lt;eos&gt; and use that eos token during training to teach the model when it should stop. Then during inference, we can use that token to recognize when the model is done responding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos_token_id: 50277\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'eos_token':'<eos>'})\n",
    "print('eos_token_id:',tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_LEN = 256  # 256 accounts for about 96% of the data in the alpaca dataset\n",
    "\n",
    "def tokenize(prompt, tokenizer,add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt+\"<eos>\",  # add the end-of-stream token\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": result[\"input_ids\"],\n",
    "        \"attention_mask\": result[\"attention_mask\"],\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a quick try and note the <eos> token id at the end of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5801, 627, 50277], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('hi there<eos>')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "When finetuning your model the dataset that you chose has to be aligned with your downstream task. We're using popular Instruction Following Dataset, called Alpaca. For convenience, I have a copy of the alpaca dataset that has been cleaned published on the HuggingFace hub. We can just download it and access it from cache using the load_dataset API shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration johnrobinsn--alpaca-cleaned-59cdb2a1c179039a\n",
      "Found cached dataset json (/mnt2/hfcache/johnrobinsn___json/johnrobinsn--alpaca-cleaned-59cdb2a1c179039a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba95a9edd5a14de79740c9546609f2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 51942\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "data = load_dataset(dataset)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the dataset consists of 51,942 rows with the following features ['instruction','input','output'].  Let's take a look at one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Identify the odd one out.',\n",
       " 'input': 'Twitter, Instagram, Telegram',\n",
       " 'output': 'Telegram'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see an item that includes an 'instruction' to direct our model.  An optional 'input' which provides context to the instruction.  And then an expected output for the model.\n",
    "\n",
    "Our goal in finetuning our model is to use this dataset to train our model to \"behave\" in a similar way.  Given an instruction respond with an appropriate response generalizing to the knowledge already encoded in the base model.\n",
    "\n",
    "But we can't directly use this JSON object to train our model.  Our model can only process an ordered sequence of tokens that represent words.  So we use a \"prompt template\" to convert each of these JSON objects in our dataset into a sequence of words.  The prompt template follows a consistent pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what what our example looks like when \"templatized\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the odd one out.\n",
      "\n",
      "### Input:\n",
      "Twitter, Instagram, Telegram\n",
      "\n",
      "### Response:\n",
      "Telegram\n"
     ]
    }
   ],
   "source": [
    "print(generate_prompt(data['train'][5]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact wording of the template is somewhat arbitrary.  It's more of a consistent pattern that after training will drive the model into responding similarly when exposed to a similar prompt.  You should be able to pick out the \"instruction\", \"input\", and \"output\" from the example.  \n",
    "\n",
    "It is important that the output from the dataset is at the end of templatized prompt, since at inference time we will only provide the prompt up to **but not including the output**.  We'll expect our model to respond to our instruction on its own."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split out a validation dataset from our training dataset so that we can track how well the finetuning process is learning to generalize to unseen prompts and so that we make sure we're only checkpointing our model when the validation loss is improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /mnt2/hfcache/johnrobinsn___json/johnrobinsn--alpaca-cleaned-59cdb2a1c179039a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-db565d6b8daf9bdc.arrow and /mnt2/hfcache/johnrobinsn___json/johnrobinsn--alpaca-cleaned-59cdb2a1c179039a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-6b7358b8f2ab7506.arrow\n"
     ]
    }
   ],
   "source": [
    "VAL_SET_SIZE = 2000  # we set aside 2000 items from our dataset for validation during training\n",
    "\n",
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=VAL_SET_SIZE, shuffle=True, seed=42\n",
    ")\n",
    "train_data = train_val[\"train\"]\n",
    "val_data = train_val[\"test\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the training dataset and the validation dataset by running the data through the prompt templating process and then by tokenizing the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8277ae01dda94e129310e5b09fc8fab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49942 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd6a72fcfa248e59d8c7619d6e38c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x), tokenizer))\n",
    "val_data = val_data.shuffle().map(lambda x: tokenize(generate_prompt(x), tokenizer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Configure the Model for Training\n",
    "\n",
    "Load the specified RedPajama base model from the HuggingFace hub.\n",
    "\n",
    "_Note: Llama, Redpajama and other decoder-only models are supported by the AutoModelForCausalLM class.  But for encoder-decoder models such as the [**google/t5**](https://huggingface.co/google/flan-t5-xxl) models you'll need to use the AutoModelForSeq2SeqLM class and the training details are a little bit different.  Here is a [similar notebook](https://github.com/johnrobinsn/flan_ul2/blob/main/train-peft-flan-ul2-int8-alpaca.ipynb) for finetuning t5* models._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for model:  togethercomputer/RedPajama-INCITE-Base-7B-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/jr/anaconda3/envs/redpj/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/jr/anaconda3/envs/redpj/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18b1039a9cd4d189ad190de7f1f699d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "print(\"Loading model for model: \", model_name[0])\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name[0],\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can prepare our model for the LoRA int-8 training using the HF peft library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4194304 || all params: 6861496320 || trainable%: 0.06112812430977155\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config \n",
    "lora_config = LoraConfig(\n",
    " r= 8, \n",
    " lora_alpha=16,\n",
    " target_modules=[\"query_key_value\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the Lora Adapters into the model notice the significant reduction in the number of trainable paramters.\n",
    "\n",
    "We'll leverage the training loop from the transformers library since it does a pretty good job with handling the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "eval_steps = 200\n",
    "save_steps = 200\n",
    "logging_steps = 20\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=3e-4,\n",
    "        logging_steps=logging_steps,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_steps=save_steps,\n",
    "        output_dir=output_dir,\n",
    "        report_to=report_to if report_to else \"none\",\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        auto_find_batch_size=True\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Run the training loop.  This will take quite a while on a Titan RTX the 3B will take X and the 7B took Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jr/anaconda3/envs/redpj/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f93716799b43c78bd2a623309f6e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18729 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jr/anaconda3/envs/redpj/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6996, 'learning_rate': 0.0002996796411981419, 'epoch': 0.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain() \n",
      "File \u001b[0;32m~/anaconda3/envs/redpj/lib/python3.9/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/redpj/lib/python3.9/site-packages/accelerate/utils/memory.py:124\u001b[0m, in \u001b[0;36mfind_executable_batch_size.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo executable batch size found, reached zero.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    123\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m function(batch_size, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    125\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    126\u001b[0m     \u001b[39mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "File \u001b[0;32m~/anaconda3/envs/redpj/lib/python3.9/site-packages/transformers/trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1904\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1905\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1906\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1907\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1908\u001b[0m ):\n\u001b[1;32m   1909\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1910\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/redpj/lib/python3.9/site-packages/transformers/trainer.py:2663\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2661\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2662\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2663\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2665\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/anaconda3/envs/redpj/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/redpj/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Trained Adpater Model to Disk\n",
    "\n",
    "Now that we've trained the model we'll want to save our weights.  First I demonstrate how to save them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Save our LoRA model & tokenizer results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave_pretrained(peft_name)\n\u001b[1;32m      3\u001b[0m tokenizer\u001b[39m.\u001b[39msave_pretrained(peft_name)\n\u001b[1;32m      5\u001b[0m \u001b[39m# if you want to save the base model to disk call\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# trainer.model.base_model.save_pretrained(peft_model_id)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Save our LoRA model & tokenizer results\n",
    "trainer.model.save_pretrained(peft_name)\n",
    "tokenizer.save_pretrained(peft_name)\n",
    "\n",
    "# if you want to save the base model to disk call\n",
    "# trainer.model.base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push the Trained Adapter Model to the HuggingFace Hub\n",
    "\n",
    "Even better than saving your trained weights to disk you can push them up the HuggingFace Hub.  This makes it super easy to share your trained adapter with others or to setup your model for inference on other devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b2b62677c94ffdbcd018f3c18691eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -Uqq huggingface_hub\n",
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't already have the git extensions for large file storage you might have to install it now.\n",
    "# Here is how you can do this for Linux from the shell.  For other OSs please refer to the git-lfs documentation.\n",
    "# sudo apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070fedcce2ad46a586a0065c2cc14c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216bdfbdfb7a4382b87a4932f9d43556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/10.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/johnrobinsn/redpj3B-lora-int8-alpaca/commit/7c83117a4ee0b332372bd605a486a34373073335', commit_message='Upload tokenizer', commit_description='', oid='7c83117a4ee0b332372bd605a486a34373073335', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = f'{huggingface_hub.whoami()[\"name\"]}/{peft_name}'\n",
    "trainer.model.push_to_hub(repo_id)\n",
    "tokenizer.push_to_hub(repo_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You chould be able to check out HuggingFace and see your LoRA Adapter Model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free Up Memory\n",
    "\n",
    "Since we likely used a lot of memory during training and we'll need that memory back to try the model out we take a few steps to free up VRAM here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "config = None\n",
    "model = None\n",
    "tokenizer=None\n",
    "trainer=None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate\n",
    "Here we'll try out the model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50432, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear8bitLt(in_features=2560, out_features=7680, bias=True)\n",
       "          (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
       "          (dense_4h_to_h): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=2560, out_features=50432, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load peft config for pre-trained checkpoint etc. \n",
    "#peft_model_id = \"results\"\n",
    "#config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name[0],\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name[1])\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.add_special_tokens({'eos_token':'<eos>'})\n",
    "# tokenizer.eos_token_id = 2\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import GenerationConfig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the prompt template we'll use for inference.\n",
    "\n",
    "_Note: It's important that it's identical to one we used for training above, but it omits the \"output/response\" as our model will generate that for us._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Response:\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small utility function that let's us easily prompt our model with an instructin and an optional input.  It handles templating the prompt, tokenizing the templatized prompt, decoding the result and then finally stripping off the prompt from the response and just leaving us with the model response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(instruction,input=None,maxTokens=256):\n",
    "    prompt = generate_prompt({'instruction':instruction,'input':input})\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    outputs = model.generate(input_ids=input_ids, max_new_tokens=maxTokens, \n",
    "                             do_sample=True, top_p=0.9,pad_token_id=tokenizer.eos_token_id,\n",
    "                             forced_eos_token_id=tokenizer.eos_token_id)\n",
    "    outputs = outputs[0].tolist()\n",
    "    # Stop decoding when hitting the EOS token\n",
    "    if tokenizer.eos_token_id in outputs:\n",
    "        eos_index = outputs.index(tokenizer.eos_token_id)\n",
    "        decoded = tokenizer.decode(outputs[:eos_index])\n",
    "        # Don't show the prompt template\n",
    "        sentinel = \"### Response:\"\n",
    "        sentinelLoc = decoded.find(sentinel)\n",
    "        if sentinelLoc >= 0:\n",
    "            print(decoded[sentinelLoc+len(sentinel):])\n",
    "        else:\n",
    "            print('Warning: Expected prompt template to be emitted.  Ignoring output.')\n",
    "    else:\n",
    "        print('Warning: no <eos> detected ignoring output')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating using the Base Model\n",
    "\n",
    "This demonstrates the behavior of the RedPajama model with no finetuning applied.  Below I'll load the LoRA adapter that has been trained on the finetuning task to demonstrate the change in behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Write a short story in third person narration about a protagonist who has to make an important career decision. The protagonist’s character is presented from the point of view of the protagonist. The first paragraph should describe a decision the protagonist made. In the second paragraph, the reader should learn more about the protagonist and why she made this decision. In the last paragraph, the reader should learn more about what the protagonist decided.\n",
      "\n",
      "### Examples:\n",
      "Write about a character from a novel who makes an important decision. \n",
      "Write about a character from a film that makes an important decision.\n",
      "Write about a character from a television show that makes an important decision.\n",
      "\n",
      "# **Writing Prompt 13: Write a Short Story**\n",
      "\n",
      "### Instructions:\n",
      "In the following prompt, write a short story in third person narration. The story can take place in the past or in the present. Write a story that contains:\n",
      "\n",
      "- An unreliable narrator\n",
      "- A dramatic situation\n",
      "- A situation that takes place during a specific time\n",
      "\n",
      "### Response:\n",
      "Write a short story in third person narration about a character who finds an important item. The character finds the item during a specific time. The story contains the following characteristics:\n",
      "\n",
      "- An unreliable narrator\n",
      "- A dramatic situation\n",
      "- A situation that takes place during a specific time\n",
      "\n",
      "### Examples:\n",
      "Write a short story about a character who finds an important item during a specific time.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "generate('Write a short story in third person narration about a protagonist who has to make an important career decision.',maxTokens=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the LoRA Adapter\n",
    "\n",
    "As you can see the generated text doesn't seem very responsive to the prompt.  Now let's load the trained LoRA adapter and see what happens.\n",
    "\n",
    "_Note: Here you can either load up my pretrained Lora adapter from HuggingFace hub.  Or if you trained your own adapter above you can uncomment the specified line below to load your adapter from disk._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model adapter loaded\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = f'johnrobinsn/{peft_name}' # By default use my pretrained adapter weights\n",
    "#peft_model_id = peft_name # Uncomment to use locally saved adapter weights if you trained above\n",
    "\n",
    "# Load the LoRA model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model adapter loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try the same prompt again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "John had been thinking about what he should do with his life for a while. He was an ambitious man, but his job responsibilities were limited and he was unsure how to advance his career. He was torn between staying with the company he had worked for for several years or leaving to pursue his own dreams. He had a hard time making the right decision.\n",
      "\n",
      "One day, he was sitting at his desk, thinking. Suddenly, a bright idea came to him. He decided to create a blog about his own ideas and inspirational stories. He was sure he could make this work as a side hustle. He knew it would be difficult to get started, but he was determined to try.\n",
      "\n",
      "He decided to start his blog by setting up a website and putting up his first few posts. He had no idea what to write or how it would turn out. He was nervous, but he knew this was a new beginning and he was ready to try something new. \n",
      "\n",
      "As the weeks went by, he wrote about various subjects and topics. He soon realized that writing and posting on his blog was a great outlet for him. He could talk to people about the ideas and insights he had and it felt good to have something to focus on. \n",
      "\n",
      "One day, he received an unexpected message from someone on his blog. They had read one of his posts and they were so inspired by it that they were inspired to pursue their own ideas. They wanted to connect with John\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "generate('Write a short story in third person narration about a protagonist who has to make an important career decision.',maxTokens=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Few More Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neil Armstrong was the first man to walk on the moon and he was born in 1930 in Ohio, USA. He was an American astronaut who was part of NASA’s Apollo 11 mission to the Moon. He took the first steps of the first humans on the Moon’s surface and uttered the famous phrase “One small step for a man, one giant leap for mankind”.\n"
     ]
    }
   ],
   "source": [
    "generate('Who was the first man to walk on the moon and tell me where he was born.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we provide not only an instruction but also provide some context for the instruction which is a list of possible answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Telegram is the odd one out because it is primarily used for group messaging rather than personal broadcasting. Other two social media platforms focus on sharing individual posts to large audiences.\n"
     ]
    }
   ],
   "source": [
    "generate('Identify the odd one out and explain why.','Twitter, Instagram, Telegram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A little grey and black, \n",
      "A tiny speck of fur,\n",
      "So tiny and dainty, \n",
      "A perfect cat for me. \n",
      "\n",
      "Lounging on the back of the sofa, \n",
      "A purr that's soothing,\n",
      "A tail that twirls, \n",
      "A cat that's purring,\n",
      "A cat that's purring. \n",
      "\n",
      "A little kitty that loves,\n",
      "A soft purr that's so sweet,\n",
      "A purr that's so purr,\n",
      "A purr that's purr. \n",
      "\n",
      "A purr that fills up the air, \n",
      "A purr that's purr,\n",
      "A purr that's purr,\n",
      "A purr that's purr.\n"
     ]
    }
   ],
   "source": [
    "generate('Write a poem about about a cat',maxTokens=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meh.  Not that great... But Llama doesn't seem to be very good at poetry either in my experience.  Would be worthwhile to see how the larger RedPJ models fair here...   Still lot's of fun probing the limits of what works well and what doesn't."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "I hope you've enjoyed this quick tour of finetuning.  I've also included a \"cleaned\" version of this notebook in the github repo without the blog narrative.\n",
    "\n",
    "If you'd like to try your hand at a different finetuning task.  You could give summarization a try.  Please check out the [samsum](https://huggingface.co/datasets/samsum) summarization dataset on HF.  Primarily, you'll need to adjust the prompt templates during training and inference.\n",
    "\n",
    "Please **like** my content on twitter, [@johnrobinsn](https://twitter.com/johnrobinsn)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4eddb440959a89d6755541b4ddad18aa547193e63575f384aa5d9491d8be2537"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
